{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we have extracted relevant features from the GPS trajectories and in this notebook we will try to classify the modality of the trajectories with these features. \n",
    "\n",
    "Since the processing can take up a few hours depending on the computational power, the processed data is provided for you. \n",
    "\n",
    "It can be downloaded from google drive; https://drive.google.com/open?id=0B22kg5oTwAn-Q0xOVDhRMVNhMDQ\n",
    "and it is also passed around in memory sticks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8cbf007e3780>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdf_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mlist_df_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdf_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_df_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#Remove all rows, which contain NaN values in these columns:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    210\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#The column containing the labels is not exactly in the clean format we want to have it in. \n",
    "#Some labels have commas at the begin, end and double commas in the middle, \n",
    "#so lets make a function which cleans these labels.\n",
    "def clean_label(label):\n",
    "    return label.lstrip(',').rstrip(',').replace(',,', ',')\n",
    "\n",
    "INPUT_FOLDER = '../processed_data/'\n",
    "headers_metadf = ['trajectory_id', 'start_time', 'end_time', 'v_ave', 'v_med', 'v_max', 'v_std', 'a_ave', 'a_med', 'a_max', 'a_std', 'labels']\n",
    "\n",
    "#Lets load all of the processed data, containing the features of all trajectories into one single dataframe. \n",
    "#The easiest way to do this is to load all of the into a list and concatenate them.\n",
    "list_df_metadata = []\n",
    "for file in glob.glob(INPUT_FOLDER + \"*_metadata.csv\"):\n",
    "    df_metadata = pd.read_csv(file, index_col=0)\n",
    "    list_df_metadata.append(df_metadata)\n",
    "df_metadata = pd.concat(list_df_metadata)\n",
    "\n",
    "#Remove all rows, which contain NaN values in these columns:\n",
    "df_labeled = df_metadata.dropna(subset=['v_ave','v_med','v_max', 'v_std', 'a_ave', 'a_med', 'a_max', 'a_std', 'labels'])\n",
    "\n",
    "#Clean the labels-column\n",
    "df_labeled.loc[:,'labels'] = df_labeled['labels'].apply(lambda x: clean_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the trajectories:\n",
    "\n",
    "Most of the trajectories are not labeled. \n",
    "\n",
    "Of the labeled trajectories around 60% contains a single modality. The other trajectories are multi-modal [walk -> bus -> train, etc] and for now we will not take them into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_labeled' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6bababc92ad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_labeled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Example of trajectory labels:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_labeled' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "all_labels = df_labeled['labels'].unique()\n",
    "print(\"Example of trajectory labels:\")\n",
    "for label in all_labels[0:5]:\n",
    "    print(label)\n",
    "\n",
    "#We can filter out single modal trajectories by taking the labels which do not contain a comma:\n",
    "single_modality_labels = [elem for elem in all_labels if ',' not in elem]\n",
    "\n",
    "df_single_modality = df_labeled[df_labeled['labels'].isin(single_modality_labels)]\n",
    "\n",
    "print(\"\\nTotal number of trajectories: {}\".format(len(df_metadata)))\n",
    "print(\"Total number of labeled trajectories: {}\".format(len(df_labeled)))\n",
    "print(\"Total number of single modality trajectories: {}\".format(len(df_single_modality)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split the trajectories containing a single modality, into a 70% training set and a 30% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_single_modality' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1834734febe2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_single_modality\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_single_modality\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_single_modality\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_single_modality' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "mask = np.random.rand(len(df_single_modality)) < 0.7\n",
    "df_train = df_single_modality[mask]\n",
    "df_test = df_single_modality[~mask]\n",
    "\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrices containing the X and Y values of the training set (X_train, Y_train) will be used to train a classifier with. And the matrices X_test and Y_test can be used to test the accuracy of the trained classifier.\n",
    "\n",
    "Usually a dataset is split randomly into a 70% training set and a 30% test set, but this also depends on the size of the dataset. If the dataset is small, 30% could not be enough for properly testing your trained classifier. \n",
    "\n",
    "You could also split it into a 50% / 25% / 25% training set / test set / validation set.\n",
    "See: https://en.wikipedia.org/wiki/Test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d888c87d30cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mY_colnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_colnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY_colnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_colnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#The columns \n",
    "X_colnames = ['v_ave','v_med','v_max', 'v_std', 'a_ave', 'a_med', 'a_max', 'a_std']\n",
    "Y_colnames = ['labels']\n",
    "\n",
    "X_train = df_train[X_colnames].values\n",
    "Y_train = np.ravel(df_train[Y_colnames].values)\n",
    "X_test = df_test[X_colnames].values\n",
    "Y_test = np.ravel(df_test[Y_colnames].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of data ready and in the format we want to have it in, lets start with the classification part. \n",
    "\n",
    "For classification of the single modal trajectories, we will use three classifiers in the scikit-learn library. \n",
    "\n",
    "## 1. Random Forest\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "\n",
    "## 2. Logistic Regression\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "http://ataspinar.com/2016/03/28/regression-logistic-regression-and-maximum-entropy/\n",
    "\n",
    "\n",
    "## 3. Support Vector Machines.\n",
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "https://youtu.be/3liCbRZPrZA\n",
    "\n",
    "\n",
    "### Also see:\n",
    "\n",
    "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "http://ciml.info/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators = 18)\n",
    "logreg_classifier = LogisticRegression()\n",
    "svm_classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9cf46770e8e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Random Forest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrf_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mt_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt_diff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_end\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_start\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Random Forest\n",
    "t_start = time.clock()\n",
    "rf_classifier.fit(X_train, Y_train)\n",
    "t_end = time.clock()\n",
    "t_diff = t_end - t_start\n",
    "\n",
    "train_score = rf_classifier.score(X_train, Y_train)\n",
    "test_score = rf_classifier.score(X_test, Y_test)\n",
    "y_pred_rf= rf_classifier.predict(X_test)\n",
    "print(\"trained Random Forest in {:.2f} s.\\t Score on training / test set: {} / {}\".format(t_diff, train_score, test_score))\n",
    "\n",
    "#Logistic Regression\n",
    "t_start = time.clock()\n",
    "logreg_classifier.fit(X_train, Y_train)\n",
    "t_end = time.clock()\n",
    "t_diff = t_end - t_start\n",
    "\n",
    "train_score = logreg_classifier.score(X_train, Y_train)\n",
    "test_score = logreg_classifier.score(X_test, Y_test)\n",
    "y_pred_logreg = logreg_classifier.predict(X_test)\n",
    "print(\"trained Logistic Regression in {:.2f} s.\\t Score on training / test set: {} / {}\".format(t_diff, train_score, test_score))\n",
    "\n",
    "#Linear SVM\n",
    "t_start = time.clock()\n",
    "svm_classifier.fit(X_train, Y_train)\n",
    "t_end = time.clock()\n",
    "t_diff = t_end - t_start\n",
    "\n",
    "train_score = svm_classifier.score(X_train, Y_train)\n",
    "test_score = svm_classifier.score(X_test, Y_test)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "print(\"trained SVM Classifier in {:.2f} s.\\t Score on training / test set: {} / {}\".format(t_diff, train_score, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the accuracy of RF classifier\n",
    "The most accurate classifier is the Random Forest classifier, with an accuracy of 78 % on the test set. \n",
    "Although this is already quiet high, lets see how we can improve it even more. \n",
    "\n",
    "To be able to do this, first we need to understand what this average accuracy of 78% consists of.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "- print the number of entries of each modality in the dataset.\n",
    "- print the f1-score per class within the test set. \n",
    "\n",
    "hint: the metrics module of the scikit-learn library contains a lot of methods which can be used to evaluate the performance of your classifier:\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are not enough entries of a specific class in the dataset, the classifier will difficulties of finding a general rule which can correctly model it. This will lower the overall accuracy of the classifier, so lets remove all of the modalities which have less than 10 entries in 'df_single_modality'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also have seen that there are entries which are labeled as a different modality, although the behaviour will be approximately the same (for example car vs taxi). Incorrectly classifying these entries will also lower the accuracy of the classifier. Which of the existing labels in df_single_modality can be combined into one label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new and improved df_single_modality:\n",
    "- generate training and test sets\n",
    "- train the three classifiers and determine if the accuracy has improved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the three classifiers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluate the performance of each mdoality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
